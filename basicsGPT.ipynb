{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKt9xmDGF2jdKI9/r+w1sE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannaofficial/GPT_using_Transformer/blob/main/basicsGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSlwFvxBNHqV",
        "outputId": "a5c0c248-67c3-4ba4-ad39-2b6bcc987768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-10 08:10:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-10-10 08:10:30 (25.2 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt','r', encoding='utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "FRn97gNYNX0_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"len of char: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA0wDgYPNmav",
        "outputId": "be0bef61-4dbd-4eb8-f200-ec66d0497264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of char: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UC7882PGNuZj",
        "outputId": "af7ff92a-f14f-4367-a988-dd11219fc7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka8k9otINya0",
        "outputId": "b88db71b-9ed7-4244-b30c-fa0d40cdc89b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ctoi = {ch: i for i,ch in enumerate(chars) }  #character to index\n",
        "itoc = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [ctoi[c] for c in s]\n",
        "decode = lambda i: ''.join([itoc[index] for index in i])"
      ],
      "metadata": {
        "id": "TvW4-SLDODm-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encode(\"hi hanna\"))\n",
        "print(decode(encode(\"hi hanna\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9uof2rJPJDU",
        "outputId": "d30022af-9a3d-48db-d113-e12768a67ab6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 1, 46, 39, 52, 52, 39]\n",
            "hi hanna\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ZENigUgEQQml"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "XHGOWmRqAw8S"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data =torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyeKEp5mQXBI",
        "outputId": "355bef3b-75f1-4476-ac8f-bc08f05508aa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "P84RSk8VR4xS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ix = torch.randint(100, (4,))\n",
        "print(ix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSUn0fbbYBA7",
        "outputId": "d36635c4-b4bf-4d54-859b-3f16792bb2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([66, 77, 44, 34])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split =='train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x.to(device),y.to(device)\n",
        "\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "# print('inputs:')\n",
        "# print(xb.shape)\n",
        "# print(xb)\n",
        "# print(xb)\n",
        "# print(yb.shape)\n",
        "# print(yb)\n",
        "\n",
        "# print('----')\n",
        "\n",
        "# for b in range(batch_size):\n",
        "#   for t in range(block_size):\n",
        "#     context = xb[b, :t+1]\n",
        "#     target = yb[b, t]\n",
        "#     print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XoRg0_XfSU4O",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "       X,Y = get_batch(split)\n",
        "       logits, loss = model(X, Y)\n",
        "       losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "Dye1l_ncBGBh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "eval_iters = 200\n",
        "n_embed = 384\n",
        "n_head=6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "learning_rate = 3e-4"
      ],
      "metadata": {
        "id": "RudzgOZqwwf5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "`self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))`<br/><br/>\n",
        "In PyTorch, register_buffer is a method used to register a tensor as part of a module's state, but not as a learnable parameter. This means the tensor will be saved and moved with the model (e.g., to GPU or CPU) but won't be updated during training.\n",
        "<br/>**Why Use register_buffer?**<br/>\n",
        "Buffers are useful for storing constant tensors or intermediate results that are essential for the model's computations but shouldn't be modified by the training process. Examples include:\n",
        "\n",
        " Masks: Like the tril tensor in your code, which is used to prevent the model from \"looking ahead\" in sequence data."
      ],
      "metadata": {
        "id": "5C-KJOsKuZCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Broadcasting?\n",
        "Broadcasting refers to the ability of PyTorch to perform arithmetic operations on tensors of different shapes by implicitly expanding them to compatible shapes without actually copying the data. This enables element-wise operations between tensors that would otherwise be incompatible."
      ],
      "metadata": {
        "id": "2b-75nCy6b3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "      super().__init__()\n",
        "      self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "      self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "      self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "      self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "      B,T,C = x.shape\n",
        "      k = self.key(x)\n",
        "      q = self.query(x)\n",
        "      wei = q@k.transpose(-2, -1)*C**-0.5\n",
        "      wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "      wei = F.softmax(wei, dim=-1)\n",
        "      wei = self.dropout(wei)\n",
        "      v = self.value(x)\n",
        "      out = wei @ v\n",
        "      return out\n",
        "\n",
        "class  MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "      super().__init__()\n",
        "      self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "      self.proj = nn.Linear(n_embed, n_embed)\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "      out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "      out = self.proj(out)\n",
        "      out = self.dropout(out)\n",
        "      return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "      super().__init__()\n",
        "      self.net = nn.Sequential(nn.Linear(n_embed, 4*n_embed),\n",
        "                               nn.ReLU(),\n",
        "                               nn.Linear(4*n_embed, n_embed),\n",
        "                               nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "       return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embed, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embed//n_head\n",
        "    self.sa =  MultiHeadAttention(n_head,head_size)\n",
        "    self.ff = FeedForward(n_embed)\n",
        "    self.ln1 = nn.LayerNorm(n_embed)\n",
        "    self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))   #skip connection\n",
        "    x = x + self.ff(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embed)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=6) for _ in range(n_layer)])\n",
        "        self.ln_final = nn.LayerNorm(n_embed)\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B,T = idx.shape\n",
        "        idx = idx.to(device)\n",
        "        tok_embed = self.token_embedding_table(idx) #BTC  batch block(time) embeddingvector size(vocab size)   here c(channel) is embed_size\n",
        "\n",
        "        pos_embed = self.position_embedding(torch.arange(T,device=device)) # t c\n",
        "        x = tok_embed + pos_embed # B T C\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.lm_head(x)   #B T C=vocab_size   we use embed_size to reduce parameter and it's efficiency\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B,T,C = logits.shape\n",
        "          logits = logits.view(B*T,C)  #pytorch want inout in this way\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "      idx = idx.to(device)\n",
        "      for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:,-block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "\n",
        "      return idx\n",
        "model = BigramLanguageModel(vocab_size).to(device)\n",
        "xb = xb.to(device)\n",
        "yb = yb.to(device)\n",
        "logits,loss = model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(model.generate(idx=torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG3-5M5nal8o",
        "outputId": "64cef6fb-7eb8-4508-bc51-d4b3bcc34657"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16384, 65])\n",
            "tensor(4.2823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "?p-DRCUZ maQJr?F,E.HpxNDzdKHEv$fsA3WW gE,CVgX\n",
            "nO,repG AQj!\n",
            ",H3-fs?',ycR,SRinHPdGdNSwOkmVRMyFwLQzlLX'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "kXgvHROz8qyf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(5000):\n",
        "  # if iter%100 == 0:\n",
        "  #   losses = estimate_loss()\n",
        "  #   print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "  xb,yb = get_batch('train')\n",
        "  logits, loss = model(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuXONGdv7EPB",
        "outputId": "51c88057-a37d-488a-b0ea-37dfd3fe3283"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2753785848617554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(model.generate(idx=torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u88KwijZ-8KV",
        "outputId": "beab76bd-3d09-4fac-d23d-0dc1353c0c75"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "To great the father withoughts smbly may king.\n",
            "\n",
            "KING HENRY VI:\n",
            "Yes, forst thou art in true; but I need nor death\n",
            "As I can putled by mew; but this be imagined\n",
            "Talk that France, for aumerless a short\n",
            "lawful show as those Lord Forth's life;\n",
            "And thither is the time foe:\n",
            "For there beares a kiss this master tender,\n",
            "I would be slay like no medwell on the sequee\n",
            "Of tumon woe\n",
            "That that beblame bald to us.\n",
            "\n",
            "SICINIUS:\n",
            "Return,\n",
            "You live,\n",
            "I warranted you fair me: but, which I have not\n",
            "To leaved him, sperce him to all deportunate\n",
            "And therefore: no first reesolve short. what is it\n",
            "\n",
            "First Consptivian:\n",
            "I pacest being fair, washing with he hide counsell,\n",
            "But did them altermity,\n",
            "I'll xoured man kiss respectal of Virts,\n",
            "That pretty fires with her prince father,\n",
            "And she was the mighty sins of the blood,\n",
            "Warwick but to the heglish'd whereinous he,\n",
            "Becomen begins ere is a blow.\n",
            "\n",
            "Gentleman:\n",
            "My lord, in court weary hands!\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Best, being flows thy descervice thing eye\n",
            "To help the services plumant of York.\n",
            "\n",
            "KING HENRY VI:\n",
            "Why, fare comes lie, Richard, did I to alo.\n",
            "\n",
            "RICHARD:\n",
            "His name! in my humice! this noble more mast\n",
            "Rainsal to our service. Beseech you I do.\n",
            "\n",
            "YORK:\n",
            "JOHN OF GAUNT:\n",
            "Well, chush! Richard; solaths, conscience,\n",
            "May do I am the find,\n",
            "Unless, Flave my deservice to Boes is vitifate.\n",
            "I thought grant I was waten was a well.\n",
            "You are there gates bring them bands,\n",
            "From the flesh of him: for worse these name;\n",
            "And you have sometimes, and his public,\n",
            "A prince fell teer where he would daspeather\n",
            "Un'd in them; two welcome. Somer Aufidius\n",
            "senares, tell him spring of their suitor,\n",
            "Will mister aeask apaborn\n",
            "To put his fair, his love sensely. How's breast!\n",
            "Silvery comfort, may serve me. But, so madam,\n",
            "I am knownly, like of me; when they Camillo,\n",
            "When was the gastes? they are so for their\n",
            "Marriary asces; but so; who deep, is it easier seens,\n",
            "As easier his may prayes to our name sance\n",
            "Hath sorrunly in the aid, and to the wetver\n",
            "Threatens of my knaving is for pasting.\n",
            "Tormer, Their Plant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **THE mathematics of Self Attention**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "dG7OMwc7o5Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhMGtvqYpGDG",
        "outputId": "6fa806dc-5a78-4b23-9d9f-0f1a4fc7f6e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b, :t+1]\n",
        "    xbow[b,t] = torch.mean(xprev,0)"
      ],
      "metadata": {
        "id": "G6vJaiITpcPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa_Iqxpap2ot",
        "outputId": "6be1e266-67d6-46ac-9c35-219c632e8bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.3596, -0.9152],\n",
              "        [ 0.6258,  0.0255],\n",
              "        [ 0.9545,  0.0643],\n",
              "        [ 0.3612,  1.1679],\n",
              "        [-1.3499, -0.5102],\n",
              "        [ 0.2360, -0.2398],\n",
              "        [-0.9211,  1.5433]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))  #triangular matrix\n",
        "a = a/torch.sum(a, 1, keepdim=True) # we are normalizing it so that we can use it to find average\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b #matrix multiplicatio\n",
        "\n"
      ],
      "metadata": {
        "id": "blHHDt1EuGB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow2 = wei@x\n",
        "torch.allclose(xbow[0], xbow2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNyrBxequ4LW",
        "outputId": "683d9387-5dbe-435e-f886-fc5ba9abe4ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)  #B T 16\n",
        "q = query(x) #B T 16\n",
        "wei = q@k.transpose(-2,-1)   # (B T 16)  @ (B 16 T) ---> (B T T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wi = F.softmax(wei, dim=-1)\n",
        "\n",
        "v  = value(x)\n",
        "out = wei@v\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHHZUkk37uVk",
        "outputId": "0bdecf4b-95a4-4cfb-a681-0b7225605beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MneAod71vfia",
        "outputId": "0facdf3b-cb53-4890-f00e-b4c4f45f88fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}